---
title: "Data Science Applied to Ag - Final Project - Design"
author: "FULL NAME STUDENT 1 & FULL NAME STUDENT 2"
format:
  html:
    code-fold: false
    embed-resources: true
    toc: true
    number-sections: true
    theme: cerulean
editor_options: 
  chunk_output_type: console
---


# Instructions  
This file contains both the **instructions** for the mid-term project and placeholders for your code. You are required to use this file to produce code, output, and answers to the questions below.  

Besides simply creating output, make sure to interpret the output. You will need to create tables and/or plots to arrive at the answers, and then comment on what you found below it.    

To get you setup, you will need to:  

  - Student #1: create a repository on your GitHub account. You can call this repository "2025dsa_finalproject_groupX", **where X is the number of your group**. Make it public, add a README, add a .gitignore for R.  
  - Student #1: follow the steps we did in class to start a new RStudio project with version control.  
  - Student #1: in your computer, create the sub-folders code, data, output, and move your data set into the `data` folder. Also, student 1 moves this current script into the `code` folder. Do a git stage, commit, push.  
  - Student #1: on GitHub, go the repository settings and invite your partner to be a collaborator in the repository. That will give them push permission.  
  - Now, both students should clone this repository on their computers like we did in class. Make sure this step works well and that you can pull and push from GitHub.  
  - Student 2, after cloning, does a git pull to get all these updates on their computer.  
  - Student 1 and 2 work together to update the README file. README files should explain what the repository is about, the goals of that project, who is working in it, and any other important details you may find.  
  
# Introduction  
Describe here the introduction of your problem. Consider this as a shortened version of your paper, where you will briefly discuss in 3-4 paragraphs what is the issue/gap in literature, and how the data you collected will help answer this gap.  

# Hypothesis and objectives  
Describe here your hypothesis, followed by your objectives. Make sure your hypothesis are testable and bold, and objectives are clear.  

# Material and Methods  
Describe here your overall material and methods as it pertains to the analysis you will conduct, including study description, site/setup description, what equipment was used, etc. just like you would in a paper. Make sure to clearly explain what was measured and how.

## Study design  
Clearly describe your study design here, including treatment design (which factors and levels, the hierarchy among them, etc.), and your experimental design (number of reps/blocks, how was randomization performed, etc.), as we talked about in class.  


## Statistical analysis  
Describe here your statistical analysis, including what type of ANOVA model you ran (based on the design above), what was your response variable, what were your explanatory variables and how were the explanatory variables treated (random or fixed). Provide your alpha level. Explain which function from which package you used to analyze this data. Explain how you checked linear model assumptions and whether or not they were met. Overall, make sure you explain in sufficient detail that, if given your data, a knowledgeable person would be able to reproduce your analysis exactly.  

# Results  
Here is where the coding is going to happen, and it will be completely up to you. Include under this section as many sub-sections (using ##) and as many chunks needed to create the analytical workflow for your analysis, starting at loading packages and data, wrangling, EDA, modeling, assumptions checking, ANOVA table, means, pairwise comparisons, and final publication-quality plot.  

Make sure to run a model that reflects your study design. Even if your study design does not include one of the designs covered in class, you are still expected to run the most appropriate model. If you need help for references, let me know.  

Before each chunk, describe the steps being performed in that chunk. For example, "Here I will load the data".  

If a chunk produces output, like printing a data frame, statistical summary, a plot, ANOVA table, etc., make sure to write text interpreting what you see and how you can/will use that information to move forward to the next steps in the workflow.  

```{r libraries, echo=FALSE}
# libraries 
library(kableExtra)
library(latexpdf)
library(readxl)
library(janitor)
library(tinytex)
library(patchwork)
library(shiny)
library(gridExtra)
library(car) # Anova function 
library(lme4) # model fitting 
library(lmerTest) # p-values
library(broom)# residuals extraction 
library(emmeans) # mean extraction 
library(multcomp) # pairwise comparison 
library(nlme) # repeated measures correlation structure

library(tidyverse)
library(gt) # great tables 
library(plotly)
library(here)
```


## 1) Read in and clean data 

Let's read in the datasets coming from the lab where each dataset should have 96 observations coming from four different time points. At the end, combine the datasets into one by assigning their respective time point, block, and treatments.   
First sampling
```{r 2024_first-sampling}
# read in data set from first sampling
sap_1 <- 
   read_csv(here("data/2024_first_sap_sampling_0423241049FL.csv")) %>% 
  # select columns I do not need
   dplyr::select(!c(1:11, nal_sample_id, date_sampled, time_sampled, sample_type, 
                    plant_type, variety, growth_stage, vigor)) %>% 
   clean_names() %>% 
  # separate column client_sample_id
   separate(client_sample_id, into = c("sample_id", "new_old"), sep = " ") %>% 
   mutate(across(c(sample_id), as.numeric)) %>%
  # arrange base on sample_id
   arrange(sample_id) %>% 
  # correct a mislabeled sample
   mutate(sample_location = case_when(sample_id == 37 ~ "S", 
                                      TRUE ~ sample_location)) %>%  # Sample 37 corresponds to soil 100% rate
  rename(n_conv_eff = nitrogen_conversion_efficiency, 
         sl = sample_location) %>% 
  # calculate days after blooming.    # first sampling April 18-19, 2024
  mutate(time_num = as.numeric(difftime(as.Date("2024-04-19"), 
                                         as.Date("2024-03-10"), 
                                         units = "days"))) %>% 
  # assign block, irrigation system, and treatment
   mutate(irr_system = case_when(sample_id %in% 1:18 & sl %in% "F"  ~ "drip", 
                                 sample_id %in% 1:24 & sl %in% "S"  ~ "drip", 
                                 .default =  "micro-sprinkler"),
          block = case_when(sample_id %in% c(1:6, 19:24) & sl %in% "F"   ~ "1", 
                           sample_id %in% c(7:12, 25:30) & sl %in% "F"   ~ "2", 
                           sample_id %in% c(1:8, 25:32) & sl %in% "S"   ~ "1", 
                           sample_id %in% c(9:16, 33:40) & sl %in% "S"   ~ "2", 
                           .default = "3"), 
          foliar_trt = case_when(sample_id %in% c(5,9,18,23,27,31) & sl %in% "F"   ~ "T1", 
                               sample_id %in% c(6,8,13,22,28,33) & sl %in% "F"  ~ "T2",
                               sample_id %in% c(1,11,17,19,30,36) & sl %in% "F"  ~ "T3",
                               sample_id %in% c(2,12,14,20,25,34) & sl %in% "F"  ~ "T4",
                               sample_id %in% c(3,7,16,21,29,32) & sl %in% "F"  ~ "T5",
                               sl %in% "S"  ~ "T7",
                               .default = "T6")) %>% 
  # relocate columns
   relocate(c(block,irr_system, foliar_trt, new_old, time_num), .after = sl) %>% 
   mutate(across(c(block,irr_system, foliar_trt, new_old), as.factor)) %>% 
   dplyr:: select(!c(sample_id, sl)) 


```

Second sampling   
```{r 2024_second-sampling}
# read in data set from second sampling
sap_2 <- 
   read_csv(here("data/2024_second_sap_sampling_0522241201FL.csv")) %>% 
  # select columns I do not need
   dplyr::select(!c(1:11, nal_sample_id, date_sampled, time_sampled, sample_type, 
                    plant_type, variety, growth_stage, vigor)) %>% 
   clean_names() %>% 
  # separate column client_sample_id
   separate(client_sample_id, into = c("sample_id", "new_old"), sep = " ") %>% 
   mutate(across(c(sample_id), as.numeric)) %>%
  # arrange base on sample_id
   arrange(sample_id) %>% 

  rename(n_conv_eff = nitrogen_conversion_efficiency, 
         sl = sample_location) %>% 
  # calculate days after blooming.  # second sampling May 20-21, 2024
   mutate(time_num = as.numeric(difftime(as.Date("2024-05-21"), # days after bloom
                                         as.Date("2024-03-10"), 
                                         units = "days"))) %>%
  # assign block, irrigation system, and treatment
   mutate(irr_system = case_when(sample_id %in% 1:18 & sl %in% "F"  ~ "drip", 
                                 sample_id %in% 1:24 & sl %in% "S"  ~ "drip", 
                                 .default =  "micro-sprinkler"),
          block = case_when(sample_id %in% c(1:6, 19:24) & sl %in% "F"   ~ "1", 
                           sample_id %in% c(7:12, 25:30) & sl %in% "F"   ~ "2", 
                           sample_id %in% c(1:8, 25:32) & sl %in% "S"   ~ "1", 
                           sample_id %in% c(9:16, 33:40) & sl %in% "S"   ~ "2", 
                           .default = "3"), 
          foliar_trt = case_when(sample_id %in% c(5,9,18,23,27,31) & sl %in% "F"   ~ "T1", 
                               sample_id %in% c(6,8,13,22,28,33) & sl %in% "F"  ~ "T2",
                               sample_id %in% c(1,11,17,19,30,36) & sl %in% "F"  ~ "T3",
                               sample_id %in% c(2,12,14,20,25,34) & sl %in% "F"  ~ "T4",
                               sample_id %in% c(3,7,16,21,29,32) & sl %in% "F"  ~ "T5",
                               sl %in% "S"  ~ "T7",
                               .default = "T6")) %>% 
  # relocate columns
   relocate(c(block,irr_system, foliar_trt, new_old, time_num), .after = sl) %>% 
   mutate(across(c(block,irr_system, foliar_trt, new_old), as.factor)) %>% 
   dplyr:: select(!c(sample_id, sl)) 
```

Third sampling   
```{r 2024_third-sampling}
# read in data set from third sampling
sap_3 <- 
   read_csv(here("data/2024_third_sap_sampling_0605241210FL.csv")) %>% 
  # select columns I do not need
   dplyr::select(!c(1:11, nal_sample_id, date_sampled, time_sampled, sample_type, 
                    plant_type, variety, growth_stage, vigor)) %>% 
   clean_names() %>% 
  # separate column client_sample_id
   separate(client_sample_id, into = c("sample_id", "new_old"), sep = " ") %>% 
   mutate(across(c(sample_id), as.numeric)) %>%
  # arrange base on sample_id
   arrange(sample_id) %>% 

  rename(n_conv_eff = nitrogen_conversion_efficiency, 
         sl = sample_location) %>% 
  # calculate days after blooming.  # third sampling June 3 -4 
  mutate(time_num = as.numeric(difftime(as.Date("2024-06-04"), # days after bloom
                                         as.Date("2024-03-10"), 
                                         units = "days"))) %>% 
  # assign block, irrigation system, and treatment
   mutate(irr_system = case_when(sample_id %in% 1:18 & sl %in% "F"  ~ "drip", 
                                 sample_id %in% 1:24 & sl %in% "S"  ~ "drip", 
                                 .default =  "micro-sprinkler"),
          block = case_when(sample_id %in% c(1:6, 19:24) & sl %in% "F"   ~ "1", 
                           sample_id %in% c(7:12, 25:30) & sl %in% "F"   ~ "2", 
                           sample_id %in% c(1:8, 25:32) & sl %in% "S"   ~ "1", 
                           sample_id %in% c(9:16, 33:40) & sl %in% "S"   ~ "2", 
                           .default = "3"), 
          foliar_trt = case_when(sample_id %in% c(5,9,18,23,27,31) & sl %in% "F"   ~ "T1", 
                               sample_id %in% c(6,8,13,22,28,33) & sl %in% "F"  ~ "T2",
                               sample_id %in% c(1,11,17,19,30,36) & sl %in% "F"  ~ "T3",
                               sample_id %in% c(2,12,14,20,25,34) & sl %in% "F"  ~ "T4",
                               sample_id %in% c(3,7,16,21,29,32) & sl %in% "F"  ~ "T5",
                               sl %in% "S"  ~ "T7",
                               .default = "T6")) %>%  
  # relocate columns
   relocate(c(block,irr_system, foliar_trt, new_old, time_num), .after = sl) %>% 
   mutate(across(c(block,irr_system, foliar_trt, new_old), as.factor)) %>% 
   dplyr:: select(!c(sample_id, sl)) 

```

Fourth sampling  
```{r 2024_fourth-sampling}
# read in data set from fourth sampling
sap_4 <- 
   read_csv(here("data/2024_fourth_sap_sampling_0619241330FL.csv")) %>% 
    # select columns I do not need
   dplyr::select(!c(1:11, nal_sample_id, date_sampled, time_sampled, sample_type, 
                    plant_type, variety, growth_stage, vigor)) %>% 
   clean_names() %>% 
  # separate column client_sample_id
   separate(client_sample_id, into = c("sample_id", "new_old"), sep = " ") %>% 
   mutate(across(c(sample_id), as.numeric)) %>%
  # arrange base on sample_id
   arrange(sample_id) %>% 

  rename(n_conv_eff = nitrogen_conversion_efficiency, 
         sl = sample_location) %>% 
  # calculate days after blooming.  # fourth samplin June 17-18
   mutate(time_num = as.numeric(difftime(as.Date("2024-06-18"), # days after bloom
                                         as.Date("2024-03-10"), 
                                         units = "days"))) %>% 
  # assign block, irrigation system, and treatment
   mutate(irr_system = case_when(sample_id %in% 1:18 & sl %in% "F"  ~ "drip", 
                                 sample_id %in% 1:24 & sl %in% "S"  ~ "drip", 
                                 .default =  "micro-sprinkler"),
          block = case_when(sample_id %in% c(1:6, 19:24) & sl %in% "F"   ~ "1", 
                           sample_id %in% c(7:12, 25:30) & sl %in% "F"   ~ "2", 
                           sample_id %in% c(1:8, 25:32) & sl %in% "S"   ~ "1", 
                           sample_id %in% c(9:16, 33:40) & sl %in% "S"   ~ "2", 
                           .default = "3"), 
          foliar_trt = case_when(sample_id %in% c(5,9,18,23,27,31) & sl %in% "F"   ~ "T1", 
                               sample_id %in% c(6,8,13,22,28,33) & sl %in% "F"  ~ "T2",
                               sample_id %in% c(1,11,17,19,30,36) & sl %in% "F"  ~ "T3",
                               sample_id %in% c(2,12,14,20,25,34) & sl %in% "F"  ~ "T4",
                               sample_id %in% c(3,7,16,21,29,32) & sl %in% "F"  ~ "T5",
                               sl %in% "S"  ~ "T7",
                               .default = "T6")) %>% 
  # relocate columns
   relocate(c(block,irr_system, foliar_trt, new_old, time_num), .after = sl) %>% 
   mutate(across(c(block,irr_system, foliar_trt, new_old), as.factor)) %>% 
   dplyr:: select(!c(sample_id, sl)) 


```


Final data set  
```{r 2024_sap-all-samples, warning=F}
# combined data set for all four sap sampling times (48 trees * 2 samples per tree = 96 samples per time point. So, 96 samples * 4 sampling times = 384 observations). However, we need to summarize the samples from `S` because there are two subsamples or trees per time point, otherwise it will be pseudoreplication. In total we need to have 336 observations. 

sap_all_samples <- 
   rbind(sap_1, sap_2, sap_3, sap_4) %>%
   mutate(time = as.factor(time_num)) %>%  # time as factor 
   rename(ammonium = "nitrogen_ammonium", 
          nitrate = "nitrogen_nitrate") %>%
   relocate(time, .before = n_conv_eff) %>% 
   # coerce response variables <0.05 ppm to NA's
   mutate(across(where(is.character), as.numeric)) %>% 
   # summarize data to have 336 observations
   group_by(block, irr_system, foliar_trt, new_old, time_num, time) %>% 
   summarise(across(where(is.numeric), \(x)  mean(x, na.rm= TRUE))) %>% 
   ungroup() %>% 
   # relocate columns for better organization
   relocate(nitrogen, ammonium, nitrate, n_conv_eff, phosphorus, potassium, calcium, kca_ratio, magnesium, sulfur, 
            iron, manganese, boron, copper, zinc, chloride, aluminum, sodium, silica, sugars, brix, ph, ec, .after = time)


```

## 2) Exploratory data analyses (EDA)

Let's confirm that:     

- Number of blocks: 3     
- Number o treatments: 7 foliar sprays (T1, T2, T3, T4, T5, T6, T7) x 2 irrigation systems (drip and micro-sprinkler) X 2 tissues (new and old) = 28   
- Number of observations: 3 x 28 = 84 observations per sample point (4 time points x 84 = 336 observations)  

```{r}
sap_all_samples %>% psych::describe() 
```
We might consider to exclude nitrate, cobalt, molybdenum, nickel, and selenium from the analysis because they have 253, 317, 307, 150, and 297 out of 336 observations, respectively. Basically, these variables have values <0.05 ppm.  

```{r glimpse(), eval=F}
sap_all_samples %>% glimpse()
```

Density plots of macronutrients, except Potassium
```{r density plot macro}
library(ggridges)

sap_all_samples %>% 
   #pivot longer so I can plot
   pivot_longer(where(is.numeric)) %>% 
   mutate(across(c(name), as.factor)) %>% 
   # plot this variables separately 
   filter(!c(name %in% c("time_num", "n_conv_eff"))) %>%
   filter(c(name %in% c("nitrogen", "ammonium", "nitrate", "phosphorus", "calcium", "magnesium", "kca_ratio", "sulfur", "chloride"))) %>% 
   #plot 
   ggplot(., aes(x = value, y = name, fill = stat((x)))) +
   geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) + # for displaying a certain threshold
   labs(x = "ppm", 
        y = "Leaf mineral nutrients") +
   scale_fill_viridis_c(option = "D") +
   theme_minimal() +
   theme(legend.position = "none") 
```

Density plots of micronutrients and trace elements 
```{r density plot micro}
library(ggridges)

sap_all_samples %>% 
   #pivot longer so I can plot
   pivot_longer(where(is.numeric)) %>% 
   mutate(across(c(name), as.factor)) %>% 
   # plot this variables separately 
   filter(!c(name %in% c("time_num", "n_conv_eff"))) %>%
   filter(!c(name %in% c("nitrogen", "ammonium", "nitrate", "phosphorus", "potassium", "calcium", "magnesium", "kca_ratio", "sulfur", "chloride"))) %>% 
   #plot 
   ggplot(., aes(x = value, y = name, fill = stat((x)))) +
   geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) + # for displaying a certain threshold
   labs(x = "ppm", 
        y = "Leaf mineral nutrients") +
   scale_fill_viridis_c(option = "D") +
   theme_minimal() +
   theme(legend.position = "none") 
```


Boxplots of Irrigation system within each block 
```{r}
sap_all_samples %>% 
   #pivot longer so I can plot
   pivot_longer(where(is.numeric)) %>% 
   mutate(across(c(name), as.factor)) %>% 
   filter(!c(name %in% c("time_num"))) %>%
   #plot 
   ggplot(aes(x= block, y = value, fill= irr_system)) + 
   geom_boxplot(position = position_dodge(width = 0.75), alpha =0.75) + 
   geom_point(position = position_dodge(width = 0.75)) + 
   geom_rug() +
   facet_wrap(.~name, scales= "free") +
   scale_fill_viridis_d() + 
   theme_bw()
```

Boxplots of foliar treatment within each irrigation system 
```{r}
sap_all_samples %>% 
   #pivot longer so I can plot
   pivot_longer(where(is.numeric)) %>% 
   mutate(across(c(name), as.factor)) %>% 
   filter(!c(name %in% c("time_num"))) %>%
   #plot 
   ggplot(aes(x= irr_system, y = value, fill= foliar_trt)) + 
   geom_boxplot(position = position_dodge(width = 0.75), alpha =0.75) + 
   #geom_point(position = position_dodge(width = 0.75)) + 
   geom_rug() +
   facet_wrap(.~name, scales= "free") +
   scale_fill_viridis_d() + 
   theme_bw()
```

Boxplots of new/old within foliar treatment 
```{r}
sap_all_samples %>% head(2)
   #pivot longer so I can plot
   pivot_longer(where(is.numeric)) %>% 
   mutate(across(c(name), as.factor)) %>% 
   filter(!c(name %in% c("time_num"))) %>%
   #plot 
   ggplot(aes(x= foliar_trt, y = value, fill= new_old)) + 
   geom_boxplot(position = position_dodge(width = 0.75), alpha =0.75) + 
   #geom_point(position = position_dodge(width = 0.75)) + 
   geom_rug() +
   facet_wrap(.~name, scales= "free") +
   scale_fill_viridis_d() + 
   theme_bw()
```


```{r view(), eval=F, echo=F}
sap_all_samples %>% View()
```

## 3) Statistical model fitting
Let's remember that `lmer()` function does not allow to specify correlation structures while `lme()` allows to fit spatial and temporal correlation structures.

Because our data includes a time variable, it should be analyzed as a **split-split-plot repeated measures**.  

- For fixed effects, let's remember that our whole-plot is irrigation system, split-plot is foliar treatment, spli-split plot is new-old leaves, and time is our repeated measures.  
- For random effects the nesting goes from biggest to lowest: replicate/whole-plot/split-plot/split-split-plot. Thus, time_num is considered to be nested within old_new leaves.  

Define the correlation structures to be used in the model and select the response variables to use
```{r}
# Define a list of correlation structures
cor_structures <- list(
  none = NULL,
  compSymm = corCompSymm(form = ~ time_num ),
  symm = corSymm(),
  #ar1 = corAR1(form = ~ time_num ),
  car1 = corCAR1(form = ~ time_num ),
  arma = corARMA(p = 1, q = 1),
  exp = corExp(form = ~ time_num ),
  gaus = corGaus(form = ~ time_num ),
  lin = corLin(form = ~ time_num ),
  ratio = corRatio(form = ~ time_num ),
  spher = corSpher(form = ~ time_num )
)

response_vars <- 
   sap_all_samples %>% 
   select(!c(time_num, nitrate, cobalt, molybdenum, nickel, selenium)) %>% 
  select(where(is.numeric)) %>% 
  names()


```

Create a function to fit the model with a given correlation structure and response variable. The function will return the AIC of the fitted model.
```{r}
options(contrasts = c("contr.sum", "contr.poly"))

# Function to fit model 
fit_model <- function(response, cor_struct) {
  formula <- as.formula(paste(response, "~ irr_system * foliar_trt * new_old * time"))
  
  tryCatch(
    lme(
      formula,
      random = ~1 | block/irr_system/foliar_trt/new_old,
      correlation = cor_struct,
      data = sap_all_samples,
      na.action = na.exclude,
      control = lmeControl(opt = "optim")
    ),
    error = function(e) NULL
  )
}

# Now map over correlation structures and response variables
results <- map(cor_structures, function(cor_struct) {
  map(response_vars, function(response) {
    model <- fit_model(response, cor_struct)
    if (!is.null(model)) {
      AIC(model)
    } else {
      NA_real_
    }
  }) %>% set_names(response_vars)
})

```

Create a table of AIC values for each response variable and correlation structure. 
```{r}
# Make a tidy table
aic_table <- results %>%
  map_dfr(~ as_tibble(.x), .id = "cor_structure")

# View the AIC table
aic_table %>% View()

```

Select the best model for each response variable based on AIC values. 
```{r}
best_model_table <- 
   aic_table %>%
  pivot_longer(
    cols = -cor_structure, 
    names_to = "response", 
    values_to = "AIC"
  ) %>%
  group_by(response) %>%
  slice_min(AIC, with_ties = FALSE) %>%
  ungroup() %>%
  select(response, cor_structure, AIC)

best_model_table %>% gt()

```

Based on the best correlation structure for each response variable, fit the final models. 
```{r}
# Make it easier to access
best_model_list <- best_model_table %>% deframe() # named vector: response -> cor_structure

# Now fit the models
best_models <- map2(
  names(best_model_list),    # responses
  best_model_list,           # best correlation structure names
  ~ {
    formula <- as.formula(paste(.x, "~ irr_system * foliar_trt * new_old * time"))
    cor_struct <- cor_structures[[.y]]  # get the correct correlation object

    tryCatch(
      lme(
        formula,
        random = ~ 1 | block/irr_system/foliar_trt/new_old,
        correlation = cor_struct,
        data = sap_all_samples,
        na.action = na.exclude,
        control = lmeControl(opt = "optim")
      ),
      error = function(e) NULL
    )
  }
)

# Name the list by response
names(best_models) <- names(best_model_list)

# Create a table of p-values
pval_table <- 
  best_models %>% 
   discard(is.null) %>%  # <-- remove NULL models first
  map(~ .x %>% Anova(type = 3) %>% tidy()) %>% # get Anova output and tidy
  map(~ .x %>% clean_names() %>% select(term, p_value)) %>% # clean names
  bind_rows(.id = "response") %>% # make a big table
  pivot_wider(names_from = response, values_from = p_value) %>% # reshape
  filter(term != "(Intercept)")  # remove intercept



```

Highlight in bold the significant p-values (p ≤ 0.05) in the table.
```{r}
# Start with basic table
tbl <- pval_table %>%
  gt() %>%
  cols_label(term = "Factor") %>%
  fmt_number(columns = everything(), decimals = 3) %>%
  tab_header(title = "", subtitle = "P-value Table") %>%
  tab_source_note("P ≤ 0.05 indicates significance. Significant values are in bold.") %>%
  tab_options(heading.align = "left", table_body.hlines.width = px(0)) %>%
  tab_style(
    style = cell_fill(color = "lightgray"),
    locations = cells_body(rows = seq(1, nrow(pval_table), 2))
  )

# Now add bold style separately for each p-value column
tbl <- reduce(
  names(pval_table)[-1],  # all p-value columns (skip 'term')
  .init = tbl,
  .f = ~ tab_style(
    .x,
    style = cell_text(weight = "bold"),
    locations = cells_body(columns = .y, rows = .data[[.y]] < 0.05)
  )
)

tbl
```

```{r}
library(broom.mixed)  # for augment()
library(ggplot2)
library(purrr)
library(patchwork)

# Discard NULL models from best_models
best_models_clean <- best_models %>% discard(is.null)

# 1. Make residuals and standardized residuals for each model
residuals_df <- 
  best_models %>%
  discard(is.null) %>%
  map(~ augment(.x) %>% 
        mutate(.stdresid = resid(.x, type = "pearson", scaled = TRUE))
      )

# 2. Plot random effects QQ-plots for each random term
# We'll define a function to do that per model
plot_random_effects <- function(model) {
  raneffects <- ranef(model)
  
  map(raneffects, ~ 
    ggplot(.x, aes(sample = `(Intercept)`)) +
      stat_qq(shape = 21, fill = "purple", size = 3, alpha = 0.7) +
      stat_qq_line() +
      labs(x = "Theoretical quantile", y = "Sample quantile") +
      theme_bw()
  )
}

# 3. Plot residuals (fitted vs residuals) and QQ plot for residuals
plot_residuals <- function(resid_df) {
  fitted_vs_resid <- 
    ggplot(resid_df, aes(.fitted, .stdresid)) +
      geom_hline(yintercept = 0, color = "red") +
      geom_point(shape = 21, fill = "purple", size = 3, alpha = 0.7) +
      geom_smooth(se = FALSE) +
      geom_hline(yintercept = c(-3, 3), linetype = "dashed", color = "red") +
      theme_bw() +
      labs(x = "Fitted values", y = "Standardized residuals")
  
  resid_qq <- 
    ggplot(resid_df, aes(sample = .stdresid)) +
      stat_qq(shape = 21, fill = "purple", size = 3, alpha = 0.7) +
      stat_qq_line() +
      theme_bw() +
      labs(x = "Theoretical quantile", y = "Sample quantile")
  
  list(fitted_vs_resid = fitted_vs_resid, resid_qq = resid_qq)
}


# 4. Now map over everything
diagnostic_plots <- map2(
  best_models_clean,
  residuals_df,
  ~ {
    randef_plots <- plot_random_effects(.x)
    resid_plots <- plot_residuals(.y)
    
    # arrange all plots using patchwork
    (randef_plots[[1]] + randef_plots[[2]] + randef_plots[[3]] + randef_plots[[4]]) / 
    (resid_plots$fitted_vs_resid + resid_plots$resid_qq)
  }
)

# 5. Store in a named list (response variables as names)
names(diagnostic_plots) <- names(best_models_clean)

# Now you can easily access, for example:
diagnostic_plots$phosphorus
diagnostic_plots$potassium


```



# Team work in GitHub  
Whether you are working with your future-self or as duos, make sure to stage, commit, and push after finishing each of the sub-sections above. When committing, write commit messages that are short and descriptive (e.g., finished wrangling).  

If you are working in duos, make sure to split the workload. I will check your collaborative work through the commit history, and if one student has contributed significantly more than the other, than that will impact grades.  

**Tip 1**: to avoid merge conflicts, make sure to **pull** first, and then start working locally. That will ensure that any changes made by your partner will be "downloaded" before you make changes to the files locally.  

**Tip 2**: make use of the Issues on this repository to set up to-do lists and assign tasks to different people. You can also use each issue/task to discuss how things should be run and get to an agreement.  

# Submitting your work  
Once you have developed all the code and answers, make sure to Render this quarto file.  

**Notes on rendering**:  

- Make sure to render your work and inspect how the final html look like.  
- If it does not look professional for whatever reason, then fix the issue, re-render it, recheck.  
- Only send me your work once your html file looks professional.  
- Some potential issues you may encounter and how to fix them:   
  - Some times your code may be creating a table output that is waaay to long and cumbersome to scroll through when rendered. If this is the case, make it more professional looking by using the `head()` function to only print the first handful of rows (instead of thousands of rows).  
  
  - **DO NOT** delete the file's heading levels (# and ##). They set up the proper heading 1 and 2 levels, and I use them to guide my grading.  
  
  - If a given chunk is also outputting warnings or messages, inhibit this behavior by changing the chunk options `message` and `warning` to `FALSE`.  
  
  - If, after rendered, 2 lines of text are connected and you wish to "break line" between them, add 2 extra spaces after the first one.  

After rendering, an .html file will be created on your `code` folder.  

Rename this file to `LASTNAME1-LASTNAME2_finalproject.html`.    
For ex., `Bastos-Mendes_finalproject.html`.

Submit the html file on eLC by **April 30th** 11:59 pm.    













  

  

